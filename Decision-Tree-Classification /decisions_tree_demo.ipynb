{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tree is :\n",
      "\n",
      "\n",
      "Reading file  ./data_files/titanic2.txt\n",
      "\n",
      "sex = female\n",
      "\tpclass = 3rd\n",
      "\t\tage = adult\n",
      "\t\t\tno\n",
      "\t\tage = child\n",
      "\t\t\tno\n",
      "\tpclass = 2nd\n",
      "\t\tage = adult\n",
      "\t\t\tno\n",
      "\t\tage = child\n",
      "\t\t\tyes\n",
      "\tpclass = 1st\n",
      "\t\tage = adult\n",
      "\t\t\tno\n",
      "\t\tage = child\n",
      "\t\t\tyes\n",
      "\tpclass = crew\n",
      "\t\tno\n",
      "sex = male\n",
      "\tpclass = 3rd\n",
      "\t\tage = adult\n",
      "\t\t\tno\n",
      "\t\tage = child\n",
      "\t\t\tno\n",
      "\tpclass = 2nd\n",
      "\t\tage = adult\n",
      "\t\t\tno\n",
      "\t\tage = child\n",
      "\t\t\tyes\n",
      "\tpclass = 1st\n",
      "\t\tage = adult\n",
      "\t\t\tno\n",
      "\t\tage = child\n",
      "\t\t\tyes\n",
      "\tpclass = crew\n",
      "\t\tno\n",
      "\n",
      "The number of nodes:  36\n",
      "\n",
      "The training accuracy is:  0.6905951840072694\n",
      "\n",
      "The leave-one-out cross validation accuracy is:  0.6901408450704225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "debug = False\n",
    "\n",
    "# Method to read file\n",
    "def read_file(path=\"./data_files/tennis.txt\"):\n",
    "    with open(path) as file:\n",
    "        reader = csv.reader(file, delimiter=\"\\t\")\n",
    "        df = list(reader)\n",
    "        return df   \n",
    "\n",
    "# Method to extract features from the testing set\n",
    "def all_examples(df):\n",
    "    attr =[]\n",
    "    first = True\n",
    "    run = 0\n",
    "    possible_attr = []\n",
    "    for d in df:\n",
    "        if run == 0: \n",
    "            first = False\n",
    "            for i in range(len(d)):\n",
    "                attr.append([])\n",
    "                possible_attr.append([])\n",
    "            run += 1\n",
    "            continue\n",
    "            \n",
    "        for i in range(len(d)):\n",
    "            attr[i].append(d[i])\n",
    "            possible_attr[i].append(d[i])\n",
    "        \n",
    "        run += 1\n",
    "    del possible_attr[-1]\n",
    "    \n",
    "    new = []\n",
    "    for i in possible_attr:\n",
    "        i = list(set(i))\n",
    "        new.append(i)\n",
    "        \n",
    "    possible_attr = new\n",
    "    outcome = attr[-1]\n",
    "    del attr[-1]\n",
    "    return outcome, attr, possible_attr\n",
    "\n",
    "# Method to extract features from the testing set\n",
    "def leave_one_example(df, test):\n",
    "    attr =[]\n",
    "    test += 1\n",
    "    first = True\n",
    "    run = 0\n",
    "    possible_attr = []\n",
    "    for d in df:\n",
    "        if run == 0: \n",
    "            first = False\n",
    "            for i in range(len(d)):\n",
    "                attr.append([])\n",
    "                possible_attr.append([])\n",
    "            run += 1\n",
    "            continue\n",
    "            \n",
    "        elif run == test:\n",
    "            run += 1\n",
    "            for i in range(len(d)):\n",
    "                possible_attr[i].append(d[i])\n",
    "            continue\n",
    "            \n",
    "        for i in range(len(d)):\n",
    "            attr[i].append(d[i])\n",
    "            possible_attr[i].append(d[i])\n",
    "        \n",
    "        run += 1\n",
    "    del possible_attr[-1]\n",
    "    \n",
    "    new = []\n",
    "    for i in possible_attr:\n",
    "        i = list(set(i))\n",
    "        new.append(i)\n",
    "        \n",
    "    possible_attr = new\n",
    "    outcome = attr[-1]\n",
    "    del attr[-1]\n",
    "    return outcome, attr, possible_attr\n",
    "\n",
    "\n",
    "# Method to partition the data in a particular node\n",
    "def partition(listVals):\n",
    "    return {val: (listVals==val).nonzero()[0] for val in np.unique(listVals)}\n",
    "\n",
    "# entrophy function\n",
    "def entropy(classifications):\n",
    "    ent = 0\n",
    "    value, counts = np.unique(classifications, return_counts=True)\n",
    "    \n",
    "    freqs = counts.astype('float')/len(classifications)\n",
    "    for prob in freqs:\n",
    "        if prob != 0.0:\n",
    "            ent -= prob * np.log2(prob)\n",
    "    return ent\n",
    "\n",
    "# Method to calculate the information gain of a particular node\n",
    "def information_gain(y, x):\n",
    "\n",
    "    res = entropy(y)\n",
    "\n",
    "    # We partition x, according to attribute values x_i\n",
    "    val, counts = np.unique(x, return_counts=True)\n",
    "    freqs = counts.astype('float')/len(x)\n",
    "    \n",
    "    if debug:\n",
    "        print(\"X \",x)\n",
    "        print(\"VAL \", val)\n",
    "        print(\"C \", counts)\n",
    "        print(freqs)\n",
    "        print()\n",
    "    \n",
    "    # We calculate a weighted average of the entropy\n",
    "    for prob, v in zip(freqs, val):\n",
    "        if debug:\n",
    "            print(\"prob \", prob)\n",
    "            print(\"v \", v)\n",
    "            print(\"x \", x)\n",
    "            print(x == v)\n",
    "            print(y[x == v])\n",
    "            print()\n",
    "        \n",
    "        res -= prob * entropy(y[x == v])\n",
    "\n",
    "    return res\n",
    "\n",
    "# Method to check if a node contains only one classification\n",
    "def has_one_class(classification):\n",
    "    return len(set(classification)) == 1\n",
    "\n",
    "# Method to calculate plurality value of a node\n",
    "def plurality_value(y):\n",
    "    val, counts = np.unique(y, return_counts=True)\n",
    "    \n",
    "    maxval = max(counts)\n",
    "    indices = [index for index, v in enumerate(counts) if v == maxval]\n",
    "    \n",
    "    # If there is a tie in the number of yes and no, then return no\n",
    "    if not all(map(lambda x: x == counts[0], counts)):\n",
    "        return 'no'\n",
    "    index = indices[0]\n",
    "    return val[index]\n",
    "\n",
    "# A recursive function that makes the decision tree by splitting every node with the best possible attribute\n",
    "def recursive_split(x, y, head, possible_attr, parent_plurality = None):\n",
    "\n",
    "    # If all examples have same classification, return the classification\n",
    "    if has_one_class(y):\n",
    "        return y[0]\n",
    "    \n",
    "    # if examples is empty, return pluraity of the parent\n",
    "    if len(y) == 0:\n",
    "        return parent_plurality\n",
    "    \n",
    "    # if attribute is empty, return the plurarity of the node\n",
    "    if len(x.T) == 0:\n",
    "        return plurality_value(y)\n",
    "    \n",
    "    plurality = plurality_value(y)\n",
    "    \n",
    "    # ----------------------------------------------------------------------\n",
    "    # Section to get attribute that gives the highest information_gain {\n",
    "    \n",
    "    \n",
    "    # Calculate a list of gains for all the attributes splits\n",
    "    gains = np.array([information_gain(y, x_attr) for x_attr in x.T])\n",
    "    if debug:\n",
    "        print(\"###########################\")\n",
    "        print(gains)\n",
    "        print(np.argmax(gains))\n",
    "        print(\"###########################\")\n",
    "    \n",
    "    # selected the attribute with highest gains (this only takes the index as that is easy to use for later use)\n",
    "    selected_attr = np.argmax(gains)\n",
    "    \n",
    "    # } ----------------------------------------------------------------------\n",
    "    \n",
    "    # If the gain is very small, it is due to floating point error in computer, so skip that and return the classification\n",
    "    if np.all(gains < 1e-6):\n",
    "        return plurality_value(np.array(y))\n",
    "    \n",
    "    if debug:\n",
    "        print(\"X \", x)\n",
    "        print(\"#########lll\")\n",
    "        print(\"Sel \", x[:, selected_attr])\n",
    "    \n",
    "    # Split using the selected attribute\n",
    "    sets = partition(x[:, selected_attr])\n",
    "    \n",
    "    # Adding attributes values that are not in the testing set examples\n",
    "    for v in possible_attr[selected_attr]:\n",
    "        if v not in list(sets.keys()):\n",
    "            sets[v] = np.array([], dtype='int64')\n",
    "    \n",
    "    if debug: print(sets)\n",
    "    \n",
    "    result = {}\n",
    "    for k, v in sets.items():\n",
    "        y_child = y.take(v, axis=0)\n",
    "        if debug: print(\"y_child \", y_child)\n",
    "        x_child = x.take(v, axis=0)\n",
    "        if debug: print(\"x_child \", x_child)\n",
    "        result[\"%s = %s\" % (head[selected_attr], k)] = recursive_split(x_child, y_child, head, possible_attr, plurality)\n",
    "        if debug: print(\"--------\")\n",
    "        \n",
    "    return result\n",
    "\n",
    "# Method to print tree and return count of number of nodes\n",
    "    \n",
    "#https://stackoverflow.com/questions/3229419/how-to-pretty-print-nested-dictionaries\n",
    "def pretty(d, printTree, indent=0):\n",
    "    num_nodes = 0\n",
    "    for key, value in d.items():\n",
    "        num_nodes += 1\n",
    "        if printTree: print('\\t' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            num_nodes += pretty(value, printTree, indent+1)\n",
    "            \n",
    "        else:\n",
    "            if printTree: print('\\t' * (indent+1) + str(value))\n",
    "            num_nodes += 1\n",
    "    return num_nodes\n",
    "\n",
    "# Method to get attribute index in a list\n",
    "def get_attr_index(attr, head):\n",
    "    return head.index(attr)\n",
    "\n",
    "# Method to get key of the child dictionary path like 'outlook = sunny' given outlook\n",
    "def get_key(attr, test, head):\n",
    "    index = get_attr_index(attr, head)\n",
    "    key = attr +' = '+ test[index]\n",
    "    return key\n",
    "\n",
    "# Method to return the prediction of decision tree\n",
    "def predict(test_set, tree, head):\n",
    "    if not isinstance(tree,dict):\n",
    "        return tree\n",
    "    attr = next(iter(tree)).split()[0]\n",
    "    child_key = get_key(attr, test_set, head)\n",
    "    child_tree = tree[child_key]\n",
    "    return predict(test_set, child_tree, head)\n",
    "\n",
    "# Method to check if the decision tree predicted the outcome correctly\n",
    "def match(test_set, tree, head):\n",
    "    return test_set[-1] == predict(test_set, tree, head)\n",
    "\n",
    "# Leave one out cross validation method\n",
    "def cross_validation(path, printTree = False):\n",
    "    ls = []\n",
    "    df = read_file(path)\n",
    "    head = df[0]\n",
    "    num_examples = len(df) -1\n",
    "    for i in range(num_examples):\n",
    "        y, x, attr = leave_one_example(df, i)\n",
    "        test = df[i+1]\n",
    "        X = np.array(x).T\n",
    "        Y = np.array(y)\n",
    "        tree = recursive_split(X, Y, head, attr)\n",
    "        num_of_nodes = pretty(tree, printTree)\n",
    "        if printTree: print(\"\\nThe number of nodes is: \", num_of_nodes, \"\\n\")\n",
    "        ls.append(match(test, tree, head))\n",
    "\n",
    "    accuracy = sum(ls)/len(ls)\n",
    "    print(\"The leave-one-out cross validation accuracy is: \", accuracy)\n",
    "    print()\n",
    "    return accuracy\n",
    "\n",
    "# Method to generate tree for all examples provided in the dataset\n",
    "def training_accuracy(path):\n",
    "    s = []\n",
    "    df = read_file(path)\n",
    "    head = df[0]\n",
    "    y, x, attr = all_examples(df)\n",
    "    X = np.array(x).T\n",
    "    Y = np.array(y)\n",
    "    tree = recursive_split(X, Y, head, attr)\n",
    "    print(\"\\nReading file \", path)\n",
    "    print()\n",
    "    num_of_nodes = pretty(tree, True)\n",
    "    print(\"\\nThe number of nodes: \", num_of_nodes)\n",
    "    print()\n",
    "    \n",
    "    # Calculating training accuracy\n",
    "    num_examples = len(df)\n",
    "    ls = []\n",
    "    for i in range(1, num_examples):\n",
    "        test = df[i]\n",
    "        ls.append(match(test, tree, head))\n",
    "    accuracy = sum(ls)/len(ls)\n",
    "    print(\"The training accuracy is: \", accuracy)\n",
    "    print()\n",
    "    return accuracy\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #if len(sys.argv) != 2:\n",
    "        #print(\"Please provide filename as the first argument: Ex. main.py pets.txt\")\n",
    "        #exit()\n",
    "        \n",
    "    filename = \"./data_files/titanic2.txt\"\n",
    "    print(\"The tree is :\\n\")\n",
    "    training_accuracy(filename)\n",
    "    \n",
    "    cross_validation(filename, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
